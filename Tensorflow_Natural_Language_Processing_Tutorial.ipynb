{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_Natural_Language_Processing_Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nianlonggu/Tensorflow-Notebooks/blob/master/Tensorflow_Natural_Language_Processing_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MQx1rfIvqqW",
        "colab_type": "text"
      },
      "source": [
        "In this example, we learn how to use tensorflow to implement a simple RNN network for the task of movie comment analysis.<br>\n",
        "A typical RNN project for text processing includes folllowing steps:<br>\n",
        "**1. Tokenization**<br/>\n",
        "**2. Padding & Truncation**<br/>\n",
        "**3. Contruct RNNs**<br/>  &ensp;  3.1 Word embedding  <br/>\n",
        "&ensp; 3.2 Add recurrent units <br/>\n",
        "&ensp; 3.3 Config the output and define loss function <br/>\n",
        "**4. Model Training & Testing** <br/>\n",
        "\n",
        "In the next, the whole procedure is illustrated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2mKN3VH5y3c",
        "colab_type": "text"
      },
      "source": [
        "### Import necessary libraraies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tX2KUhe52re",
        "colab_type": "code",
        "outputId": "8a9c771e-1370-4183-fca8-ea4ef97171e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # used for integer-tokenization of raw text\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences   #used for padding short sequences\n",
        "from keras.datasets import imdb\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieZTXYNc5oVY",
        "colab_type": "text"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGnbNwLuvccq",
        "colab_type": "code",
        "outputId": "aa38bbf8-5570-4ad3-c1b8-84dc2866df75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "try:\n",
        "  (x_train, y_train ), ( x_test, y_test) = imdb.load_data()\n",
        "except:\n",
        "  print(\"numpy version doesn't fit, use np.load manually!\")\n",
        "  imdb_data = np.load(\"/root/.keras/datasets/imdb.npz\", allow_pickle = True)\n",
        "  x_train, y_train, x_test, y_test = imdb_data[\"x_train\"], imdb_data[\"y_train\"],\\\n",
        "                                      imdb_data[\"x_test\"], imdb_data[\"y_test\"]\n",
        "\n",
        "y_train = np.expand_dims(y_train, -1)\n",
        "y_test = np.expand_dims(y_test, -1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "numpy version doesn't fit, use np.load manually!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzbDcv6GWr8h",
        "colab_type": "code",
        "outputId": "5b1591c4-4acc-4e70-c146-1a752a26f8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "print(type(x_train))\n",
        "print(type(x_train[0]))\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(\"x_train examples:\")\n",
        "print(x_train[0][:10])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'list'>\n",
            "(25000,)\n",
            "(25000, 1)\n",
            "x_train examples:\n",
            "[23022, 309, 6, 3, 1069, 209, 9, 2175, 30, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh1-0c0vCXBA",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5xQAsKNuhU7",
        "colab_type": "text"
      },
      "source": [
        "We notice that x_train is an ndarray of list, each list contains integer-tokenized value. We can use imdb.load_word_index() to get the index (integer representation) of each word, so we can convert tokenized sequence back into text sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGo1kTsmyAZN",
        "colab_type": "code",
        "outputId": "2002f986-403b-475a-c98a-377c0ce06b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "## we get the inverse_word_index\n",
        "inverse_word_index = {}\n",
        "for key in word_index.keys():\n",
        "  inverse_word_index[word_index[key]] = key\n",
        "  \n",
        "\n",
        "# define a helper function\n",
        "def sequences_to_texts(token_sequences):\n",
        "  # word indices is an ndarray of integer token list or a nested token list\n",
        "  text_list = []\n",
        "  for i in range(len(token_sequences)):\n",
        "    text_list.append( [ inverse_word_index[ids] for ids in token_sequences[i] ]   )\n",
        "  text_list = np.array(text_list)\n",
        "  return text_list\n",
        "\n",
        "\n",
        "x_train_text = sequences_to_texts(x_train)\n",
        "x_test_text =  sequences_to_texts(x_test)\n",
        "\n",
        "\n",
        "print(\"Train examples:\\n\", ' '.join(x_train_text[0]) )\n",
        "print(\"Test examples:\\n\", \" \".join(x_test_text[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 0us/step\n",
            "Train examples:\n",
            " bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i'm here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn't\n",
            "Test examples:\n",
            " i went and saw this movie last night after being coaxed to by a few friends of mine i'll admit that i was reluctant to see it because from what i knew of ashton kutcher he was only able to do comedy i was wrong kutcher played the character of jake fischer very well and kevin costner played ben randall with such professionalism the sign of a good movie is that it can toy with our emotions this one did exactly that the entire theater which was sold out was overcome by laughter during the first half of the movie and were moved to tears during the second half while exiting the theater i not only saw many women in tears but many full grown men as well trying desperately not to let anyone see them crying this movie was great and i suggest that you go see it before you judge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad3IufjN7g3H",
        "colab_type": "text"
      },
      "source": [
        "Suppose that we get the raw text first and we want to convert texts to integer token sequences, we can use Tokenizer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjF4h4NWWXlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_all_text = np.concatenate( [x_train_text, x_test_text] , axis =0)   # It is reasonable that we need to consider both training and test dataset when tokenization\n",
        "tokenizer = Tokenizer(num_words = None)\n",
        "tokenizer.fit_on_texts(x_all_text)    ## input can be ndarray or nested list\n",
        "x_train_sequence = tokenizer.texts_to_sequences( x_train_text )\n",
        "x_test_sequence = tokenizer.texts_to_sequences(x_test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR0BLLLBBqRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## we can look several examples of the word_index of this tokenizer. Note that this tokenizer.word_index is different from the imdb.get_word_index()\n",
        "# print(\"Tokenizer word index:\\n\",tokenizer.word_index )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frILJQWGCJrr",
        "colab_type": "text"
      },
      "source": [
        "In the following we still use x_train, y_train, x_test, y_test as the default sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG62KVAsCeZg",
        "colab_type": "text"
      },
      "source": [
        "### Padding and Truncation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryu6zAOyC3M_",
        "colab_type": "text"
      },
      "source": [
        "Padding and truncation is to make sure every sequence in the training or test dataset has the same time length, so that during training we can fed a batch of data instead of single sample. <br/>\n",
        "The common stragtegies of select length include:<br/>\n",
        "&ensp; 1. select length as the maximum sequence length; <br/>\n",
        "&ensp; 2. select length as a length which can cover the majority of the sequence lengths, for those senquences whose lengths exceed that threshold, just truncate them;\n",
        "\n",
        "Here we try on the 2nd method.\n",
        "\n",
        "During padding and truncation there are also 2 ways: \"pre\" or \"post\". Here we choose \"pre\", which means add 0s or truncating from the head of each sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW1K7TU8-GwC",
        "colab_type": "code",
        "outputId": "5531af5e-f9df-4162-80e5-6cf95a9b375d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "x_all_data = np.concatenate( [x_train, x_test], axis = 0 ).tolist()\n",
        "sequence_len_list = np.array([len(seq) for seq in x_all_data ])\n",
        "len_mean, len_std = np.mean(sequence_len_list), np.std(sequence_len_list)\n",
        "print(\"Sequence length mean: \", len_mean)\n",
        "print(\"Sequence length std: \", len_std)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence length mean:  233.75892\n",
            "Sequence length std:  172.91149458735703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JURI2wxCLaOC",
        "colab_type": "text"
      },
      "source": [
        "The mean of sequence length $\\mu=234$ and standard deviation $\\sigma=173$.  If we assume that the distribution of length is Gaussian, then the confidence levels of $\\mu\\pm\\sigma$, $\\mu\\pm2\\sigma$, $\\mu\\pm3\\sigma$ are 68.26%, 95.44%, and 99.74% respectively. Therefore, we can select the uniform sequence length as $\\mu+2\\sigma$, which is above around (95.44+100)/2 % = 97.72% sequences "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vksV7dtlLZef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9449b217-d470-4795-cc55-83a78a509db5"
      },
      "source": [
        "sequence_len = int(len_mean+2*len_std)\n",
        "percentage_below_sequence_length = np.sum( sequence_len_list < sequence_len  )/len(sequence_len_list)\n",
        "print(\"%.2f%%sequences have shorter length than %d\"%(percentage_below_sequence_length, sequence_len))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.95%sequences have shorter length than 579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11TP-83FPdtB",
        "colab_type": "text"
      },
      "source": [
        "After select the sequence_length, we use pad_sequences to pad and truncate all sequence from x_train and x_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a_kKsZzW5wu",
        "colab_type": "code",
        "outputId": "04df9778-def5-4daf-dcc1-084d6ec3b688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "pad = \"pre\"\n",
        "x_train_pad = pad_sequences( x_train.tolist(), maxlen = sequence_len, padding = pad, truncating = pad )  ## the input can be ndarray of lists or nested lists\n",
        "x_test_pad = pad_sequences( x_test, maxlen = sequence_len, padding = pad, truncating =pad )\n",
        "print(type(x_train_pad))\n",
        "print(x_train_pad.shape)\n",
        "print(x_train_pad[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(25000, 579)\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0 23022   309     6\n",
            "     3  1069   209     9  2175    30     1   169    55    14    46    82\n",
            "  5869    41   393   110   138    14  5359    58  4477   150     8     1\n",
            "  5032  5948   482    69     5   261    12 23022 73935  2003     6    73\n",
            "  2436     5   632    71     6  5359     1 25279     5  2004 10471     1\n",
            "  5941  1534    34    67    64   205   140    65  1232 63526 21145     1\n",
            " 49265     4     1   223   901    29  3024    69     4     1  5863    10\n",
            "   694     2    65  1534    51    10   216     1   387     8    60     3\n",
            "  1472  3724   802     5  3521   177     1   393    10  1238 14030    30\n",
            "   309     3   353   344  2989   143   130     5  7804    28     4   126\n",
            "  5359  1472  2375     5 23022   309    10   532    12   108  1470     4\n",
            "    58   556   101    12 23022   309     6   227  4187    48     3  2237\n",
            "    12     9   215]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTn0BkxA-fGc",
        "colab_type": "text"
      },
      "source": [
        "##Implement using tensorflow functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr7bfIRqRc-M",
        "colab_type": "text"
      },
      "source": [
        "###Construct RNNs\n",
        "\n",
        "Both tensorflow and keras can be used to contruct RNNs. Although keras method is much simpler, tensorflow version can better reveal the network structure. So we first use tensorflow version to construct networks, and latter try keras version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0TBrGdg-rlc",
        "colab_type": "text"
      },
      "source": [
        "#### Word Embedding\n",
        "As we showed above, each entry in x_train is a 579 vector. Each element in this vector is an integer token (word inddex) which corresponds to a single word. The task of find word embedding is to convert each integer into a float type vector. Ideally the euclidean/cosine distance between two embeddings can represent the similarity of two words, even if their integer tokens vary a lot. <br/>\n",
        "To achieve this we can define a tensor which is a look-up table. Given each integer, we can fetch a unique feature vector for this integer token. Note: **This look-up table will be also optimzed during training.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uni06Xm-zeUY",
        "colab_type": "code",
        "outputId": "dc8c89e8-c427-4ebb-c0de-10e616535211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "# first we need to define placeholder X and y\n",
        "X= tf.placeholder( tf.int32, [None, sequence_len ]  )\n",
        "y = tf.placeholder( tf.float32, [None,1] )\n",
        "\n",
        "vocabulary_size, embedding_size =  len(word_index), 8\n",
        "# This is a look-up table, the first dim is the vocabulary size, the second dim is the embedding dimension\n",
        "word_embeddings = tf.get_variable(\"word_embeddings\", [ vocabulary_size, embedding_size  ]  )  \n",
        "\n",
        "# then we use the embedding_lookup method to get the word embedding given a batch of X\n",
        "X_embedded = tf.nn.embedding_lookup( word_embeddings, X )\n",
        "\n",
        "print(X_embedded)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 579, 20), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTg-pKrga9Pv",
        "colab_type": "text"
      },
      "source": [
        "So we notice that X_embedded has a shape of [batch_size,  sequence_length,  input_dimension ]. In some cases where we do not need to do embedding first, our placeholder should also have the sahpe of [None, sequence_length, input_dimension]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpHOjsbpbdVs",
        "colab_type": "text"
      },
      "source": [
        "#### Add Recurrent Units\n",
        "Here  we add two RNN units. This represents the deep RNN structure where multiple Recurrent Units are concatenated. The first RU takes X_embedded as the inout sequence, while the second RU takes the first RU's output as input sequence. It is like two units are concatented along the network layer  but are parallel along the time axis.\n",
        "\n",
        "\n",
        "A further introduction for the  outputs, state =  tf.nn.dynamic_rnn( RNN_Cell, input_seq, dtype  )\n",
        "for BasicRNN cell, the hidden state h  equals the last (non-zero) ouput. This hidden state is just the notation \"a\" in Andrew's course\n",
        "for LSTMCell, the state is tuple,  state = ( c, h ) where the first varaible is the internal memory cell variable \"c\", and h is the hidden state varaible \"a\"\n",
        "\n",
        "\n",
        "Moreover, since hidden state h always contains the last non-zero ouput, while outputs[-1] cound be 0, especially for the case where we pad 0's to the tail of short sequences. Therefore, \"h\" may represent the \"real\" last output of different sequences. That's why it is safer to **use state[-1] as the final output of the RNN cell** for further processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0xesu9Kz1HX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "41d06547-6851-4d1c-a16a-a41bdc146e45"
      },
      "source": [
        "# num_units is the dimension of the output (the output \"a\" and the state variable \"c\")\n",
        "basic_unit_1 = tf.nn.rnn_cell.BasicLSTMCell( num_units = 8, name = \"ru1\" )\n",
        "basic_unit_2 = tf.nn.rnn_cell.BasicLSTMCell( num_units = 4, name = \"ru2\" )\n",
        "\n",
        "\n",
        "# the outputs are a sequence of outputs at each time step, while the state is a tuple which contains the last output (a) and the state info (c)\n",
        "outputs , state = tf.nn.dynamic_rnn( basic_unit_1, X_embedded, dtype = tf.float32 )\n",
        "outputs , state = tf.nn.dynamic_rnn( basic_unit_2, outputs, dtype = tf.float32 )\n",
        "\n",
        "print(outputs, state)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-7387e732d528>:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-11-7387e732d528>:6: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "Tensor(\"rnn_1/transpose_1:0\", shape=(?, 579, 8), dtype=float32) LSTMStateTuple(c=<tf.Tensor 'rnn_1/while/Exit_3:0' shape=(?, 8) dtype=float32>, h=<tf.Tensor 'rnn_1/while/Exit_4:0' shape=(?, 8) dtype=float32>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6Vs3TKUmCAy",
        "colab_type": "text"
      },
      "source": [
        "#### Config the output and define loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiSlF5algU_A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c778763e-a10e-4267-c3a8-d4e273082579"
      },
      "source": [
        "logits = tf.layers.dense( state[-1], 1 )\n",
        "pred_y = tf.nn.sigmoid( logits )\n",
        "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( labels = y, logits = logits ))\n",
        "\n",
        "accuracy = tf.reduce_mean( tf.cast( tf.equal( tf.cast( pred_y > 0.5, tf.int32), tf.cast( y, tf.int32) ), tf.float32))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1E-3).minimize(loss)\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAYF82JSqC9G",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUSId8hWr7hV",
        "colab_type": "text"
      },
      "source": [
        "First we provide some util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sw1_pR5r6I1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_next_batch(x,y, batch_size):\n",
        "  data_length = x.shape[0]\n",
        "  selected_index = np.random.choice(  data_length, batch_size, replace = False )\n",
        "  return x[selected_index], y[selected_index]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NUEv24_qFQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5254
        },
        "outputId": "97734697-5056-4d86-ec31-a36c75403b4b"
      },
      "source": [
        "batch_size = 64\n",
        "num_batches = x_train.shape[0] // batch_size\n",
        "max_epochs = 20\n",
        "\n",
        "# print(num_batches)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  \n",
        "  for epoch in range(max_epochs):\n",
        "    for batch in range( num_batches):\n",
        "      x_train_batch, y_train_batch = get_next_batch( x_train_pad, y_train, batch_size )\n",
        "      x_test_batch, y_test_batch = get_next_batch( x_test_pad, y_test, batch_size )\n",
        "      sess.run( optimizer, feed_dict={ X:x_train_batch, y:y_train_batch }  )\n",
        "      \n",
        "      if batch%10 == 0:\n",
        "        loss_train = loss.eval( feed_dict={X:x_train_batch, y:y_train_batch} )\n",
        "        acc_train = accuracy.eval(feed_dict={ X:x_train_batch, y:y_train_batch })\n",
        "        acc_test = accuracy.eval( feed_dict={ X:x_test_batch, y:y_test_batch } )\n",
        "#         pred_y_test = pred_y.eval(feed_dict={ X:x_test_batch, y:y_test_batch })\n",
        "#         pred_y_test = (pred_y_test>0.5).astype(np.int64)\n",
        "#         real_y_test = y_test_batch.astype(np.int64)\n",
        "#         acc = np.mean( pred_y_test == real_y_test )\n",
        "    \n",
        "        print(\"Epoch %.2f, Training loss: %.2f, Train Accuracy: %.2f, Test Accuracy: %.2f\"%(epoch+batch/num_batches, loss_train, acc_train*100, acc_test*100))  \n",
        "        \n",
        "      \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0.00, Training loss: 0.69, Train Accuracy: 50.00, Test Accuracy: 48.44\n",
            "Epoch 0.03, Training loss: 0.69, Train Accuracy: 50.00, Test Accuracy: 54.69\n",
            "Epoch 0.05, Training loss: 0.69, Train Accuracy: 67.19, Test Accuracy: 59.38\n",
            "Epoch 0.08, Training loss: 0.68, Train Accuracy: 79.69, Test Accuracy: 67.19\n",
            "Epoch 0.10, Training loss: 0.66, Train Accuracy: 67.19, Test Accuracy: 50.00\n",
            "Epoch 0.13, Training loss: 0.59, Train Accuracy: 70.31, Test Accuracy: 79.69\n",
            "Epoch 0.15, Training loss: 0.53, Train Accuracy: 85.94, Test Accuracy: 73.44\n",
            "Epoch 0.18, Training loss: 0.58, Train Accuracy: 76.56, Test Accuracy: 64.06\n",
            "Epoch 0.21, Training loss: 0.52, Train Accuracy: 84.38, Test Accuracy: 78.12\n",
            "Epoch 0.23, Training loss: 0.49, Train Accuracy: 79.69, Test Accuracy: 81.25\n",
            "Epoch 0.26, Training loss: 0.40, Train Accuracy: 87.50, Test Accuracy: 79.69\n",
            "Epoch 0.28, Training loss: 0.36, Train Accuracy: 87.50, Test Accuracy: 85.94\n",
            "Epoch 0.31, Training loss: 0.40, Train Accuracy: 84.38, Test Accuracy: 81.25\n",
            "Epoch 0.33, Training loss: 0.30, Train Accuracy: 92.19, Test Accuracy: 92.19\n",
            "Epoch 0.36, Training loss: 0.27, Train Accuracy: 93.75, Test Accuracy: 89.06\n",
            "Epoch 0.38, Training loss: 0.27, Train Accuracy: 89.06, Test Accuracy: 81.25\n",
            "Epoch 0.41, Training loss: 0.37, Train Accuracy: 85.94, Test Accuracy: 84.38\n",
            "Epoch 0.44, Training loss: 0.24, Train Accuracy: 93.75, Test Accuracy: 87.50\n",
            "Epoch 0.46, Training loss: 0.42, Train Accuracy: 81.25, Test Accuracy: 81.25\n",
            "Epoch 0.49, Training loss: 0.29, Train Accuracy: 89.06, Test Accuracy: 78.12\n",
            "Epoch 0.51, Training loss: 0.24, Train Accuracy: 92.19, Test Accuracy: 81.25\n",
            "Epoch 0.54, Training loss: 0.36, Train Accuracy: 84.38, Test Accuracy: 89.06\n",
            "Epoch 0.56, Training loss: 0.23, Train Accuracy: 92.19, Test Accuracy: 87.50\n",
            "Epoch 0.59, Training loss: 0.16, Train Accuracy: 93.75, Test Accuracy: 85.94\n",
            "Epoch 0.62, Training loss: 0.31, Train Accuracy: 89.06, Test Accuracy: 81.25\n",
            "Epoch 0.64, Training loss: 0.31, Train Accuracy: 92.19, Test Accuracy: 82.81\n",
            "Epoch 0.67, Training loss: 0.15, Train Accuracy: 95.31, Test Accuracy: 82.81\n",
            "Epoch 0.69, Training loss: 0.21, Train Accuracy: 93.75, Test Accuracy: 90.62\n",
            "Epoch 0.72, Training loss: 0.21, Train Accuracy: 93.75, Test Accuracy: 87.50\n",
            "Epoch 0.74, Training loss: 0.24, Train Accuracy: 92.19, Test Accuracy: 79.69\n",
            "Epoch 0.77, Training loss: 0.31, Train Accuracy: 89.06, Test Accuracy: 89.06\n",
            "Epoch 0.79, Training loss: 0.16, Train Accuracy: 93.75, Test Accuracy: 79.69\n",
            "Epoch 0.82, Training loss: 0.13, Train Accuracy: 96.88, Test Accuracy: 82.81\n",
            "Epoch 0.85, Training loss: 0.22, Train Accuracy: 90.62, Test Accuracy: 85.94\n",
            "Epoch 0.87, Training loss: 0.24, Train Accuracy: 89.06, Test Accuracy: 84.38\n",
            "Epoch 0.90, Training loss: 0.14, Train Accuracy: 96.88, Test Accuracy: 93.75\n",
            "Epoch 0.92, Training loss: 0.22, Train Accuracy: 90.62, Test Accuracy: 96.88\n",
            "Epoch 0.95, Training loss: 0.18, Train Accuracy: 95.31, Test Accuracy: 82.81\n",
            "Epoch 0.97, Training loss: 0.08, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 1.00, Training loss: 0.11, Train Accuracy: 98.44, Test Accuracy: 89.06\n",
            "Epoch 1.03, Training loss: 0.12, Train Accuracy: 93.75, Test Accuracy: 85.94\n",
            "Epoch 1.05, Training loss: 0.19, Train Accuracy: 93.75, Test Accuracy: 82.81\n",
            "Epoch 1.08, Training loss: 0.11, Train Accuracy: 95.31, Test Accuracy: 90.62\n",
            "Epoch 1.10, Training loss: 0.24, Train Accuracy: 92.19, Test Accuracy: 84.38\n",
            "Epoch 1.13, Training loss: 0.29, Train Accuracy: 89.06, Test Accuracy: 89.06\n",
            "Epoch 1.15, Training loss: 0.18, Train Accuracy: 92.19, Test Accuracy: 81.25\n",
            "Epoch 1.18, Training loss: 0.17, Train Accuracy: 92.19, Test Accuracy: 85.94\n",
            "Epoch 1.21, Training loss: 0.12, Train Accuracy: 95.31, Test Accuracy: 85.94\n",
            "Epoch 1.23, Training loss: 0.12, Train Accuracy: 96.88, Test Accuracy: 87.50\n",
            "Epoch 1.26, Training loss: 0.13, Train Accuracy: 95.31, Test Accuracy: 89.06\n",
            "Epoch 1.28, Training loss: 0.21, Train Accuracy: 90.62, Test Accuracy: 90.62\n",
            "Epoch 1.31, Training loss: 0.13, Train Accuracy: 96.88, Test Accuracy: 84.38\n",
            "Epoch 1.33, Training loss: 0.15, Train Accuracy: 96.88, Test Accuracy: 82.81\n",
            "Epoch 1.36, Training loss: 0.22, Train Accuracy: 92.19, Test Accuracy: 84.38\n",
            "Epoch 1.38, Training loss: 0.16, Train Accuracy: 92.19, Test Accuracy: 87.50\n",
            "Epoch 1.41, Training loss: 0.15, Train Accuracy: 95.31, Test Accuracy: 84.38\n",
            "Epoch 1.44, Training loss: 0.13, Train Accuracy: 95.31, Test Accuracy: 85.94\n",
            "Epoch 1.46, Training loss: 0.16, Train Accuracy: 95.31, Test Accuracy: 85.94\n",
            "Epoch 1.49, Training loss: 0.10, Train Accuracy: 98.44, Test Accuracy: 87.50\n",
            "Epoch 1.51, Training loss: 0.19, Train Accuracy: 93.75, Test Accuracy: 84.38\n",
            "Epoch 1.54, Training loss: 0.14, Train Accuracy: 95.31, Test Accuracy: 93.75\n",
            "Epoch 1.56, Training loss: 0.05, Train Accuracy: 100.00, Test Accuracy: 89.06\n",
            "Epoch 1.59, Training loss: 0.15, Train Accuracy: 95.31, Test Accuracy: 82.81\n",
            "Epoch 1.62, Training loss: 0.13, Train Accuracy: 96.88, Test Accuracy: 90.62\n",
            "Epoch 1.64, Training loss: 0.13, Train Accuracy: 96.88, Test Accuracy: 89.06\n",
            "Epoch 1.67, Training loss: 0.24, Train Accuracy: 92.19, Test Accuracy: 87.50\n",
            "Epoch 1.69, Training loss: 0.13, Train Accuracy: 95.31, Test Accuracy: 81.25\n",
            "Epoch 1.72, Training loss: 0.09, Train Accuracy: 96.88, Test Accuracy: 82.81\n",
            "Epoch 1.74, Training loss: 0.19, Train Accuracy: 93.75, Test Accuracy: 92.19\n",
            "Epoch 1.77, Training loss: 0.10, Train Accuracy: 96.88, Test Accuracy: 79.69\n",
            "Epoch 1.79, Training loss: 0.08, Train Accuracy: 96.88, Test Accuracy: 87.50\n",
            "Epoch 1.82, Training loss: 0.03, Train Accuracy: 98.44, Test Accuracy: 82.81\n",
            "Epoch 1.85, Training loss: 0.08, Train Accuracy: 96.88, Test Accuracy: 78.12\n",
            "Epoch 1.87, Training loss: 0.13, Train Accuracy: 98.44, Test Accuracy: 85.94\n",
            "Epoch 1.90, Training loss: 0.13, Train Accuracy: 96.88, Test Accuracy: 87.50\n",
            "Epoch 1.92, Training loss: 0.11, Train Accuracy: 98.44, Test Accuracy: 82.81\n",
            "Epoch 1.95, Training loss: 0.11, Train Accuracy: 95.31, Test Accuracy: 85.94\n",
            "Epoch 1.97, Training loss: 0.11, Train Accuracy: 98.44, Test Accuracy: 87.50\n",
            "Epoch 2.00, Training loss: 0.12, Train Accuracy: 96.88, Test Accuracy: 85.94\n",
            "Epoch 2.03, Training loss: 0.14, Train Accuracy: 95.31, Test Accuracy: 92.19\n",
            "Epoch 2.05, Training loss: 0.04, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 2.08, Training loss: 0.06, Train Accuracy: 98.44, Test Accuracy: 82.81\n",
            "Epoch 2.10, Training loss: 0.07, Train Accuracy: 96.88, Test Accuracy: 89.06\n",
            "Epoch 2.13, Training loss: 0.14, Train Accuracy: 95.31, Test Accuracy: 84.38\n",
            "Epoch 2.15, Training loss: 0.06, Train Accuracy: 98.44, Test Accuracy: 89.06\n",
            "Epoch 2.18, Training loss: 0.03, Train Accuracy: 100.00, Test Accuracy: 90.62\n",
            "Epoch 2.21, Training loss: 0.08, Train Accuracy: 96.88, Test Accuracy: 78.12\n",
            "Epoch 2.23, Training loss: 0.24, Train Accuracy: 92.19, Test Accuracy: 85.94\n",
            "Epoch 2.26, Training loss: 0.22, Train Accuracy: 93.75, Test Accuracy: 87.50\n",
            "Epoch 2.28, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 93.75\n",
            "Epoch 2.31, Training loss: 0.14, Train Accuracy: 95.31, Test Accuracy: 92.19\n",
            "Epoch 2.33, Training loss: 0.04, Train Accuracy: 98.44, Test Accuracy: 87.50\n",
            "Epoch 2.36, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 81.25\n",
            "Epoch 2.38, Training loss: 0.19, Train Accuracy: 92.19, Test Accuracy: 73.44\n",
            "Epoch 2.41, Training loss: 0.44, Train Accuracy: 79.69, Test Accuracy: 78.12\n",
            "Epoch 2.44, Training loss: 0.13, Train Accuracy: 93.75, Test Accuracy: 87.50\n",
            "Epoch 2.46, Training loss: 0.21, Train Accuracy: 92.19, Test Accuracy: 84.38\n",
            "Epoch 2.49, Training loss: 0.05, Train Accuracy: 98.44, Test Accuracy: 84.38\n",
            "Epoch 2.51, Training loss: 0.11, Train Accuracy: 96.88, Test Accuracy: 87.50\n",
            "Epoch 2.54, Training loss: 0.14, Train Accuracy: 96.88, Test Accuracy: 89.06\n",
            "Epoch 2.56, Training loss: 0.16, Train Accuracy: 96.88, Test Accuracy: 89.06\n",
            "Epoch 2.59, Training loss: 0.13, Train Accuracy: 92.19, Test Accuracy: 87.50\n",
            "Epoch 2.62, Training loss: 0.06, Train Accuracy: 98.44, Test Accuracy: 90.62\n",
            "Epoch 2.64, Training loss: 0.12, Train Accuracy: 96.88, Test Accuracy: 82.81\n",
            "Epoch 2.67, Training loss: 0.07, Train Accuracy: 98.44, Test Accuracy: 92.19\n",
            "Epoch 2.69, Training loss: 0.07, Train Accuracy: 96.88, Test Accuracy: 87.50\n",
            "Epoch 2.72, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 2.74, Training loss: 0.09, Train Accuracy: 96.88, Test Accuracy: 78.12\n",
            "Epoch 2.77, Training loss: 0.06, Train Accuracy: 98.44, Test Accuracy: 84.38\n",
            "Epoch 2.79, Training loss: 0.04, Train Accuracy: 98.44, Test Accuracy: 85.94\n",
            "Epoch 2.82, Training loss: 0.09, Train Accuracy: 96.88, Test Accuracy: 87.50\n",
            "Epoch 2.85, Training loss: 0.03, Train Accuracy: 100.00, Test Accuracy: 81.25\n",
            "Epoch 2.87, Training loss: 0.03, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 2.90, Training loss: 0.09, Train Accuracy: 98.44, Test Accuracy: 81.25\n",
            "Epoch 2.92, Training loss: 0.03, Train Accuracy: 100.00, Test Accuracy: 89.06\n",
            "Epoch 2.95, Training loss: 0.11, Train Accuracy: 95.31, Test Accuracy: 76.56\n",
            "Epoch 2.97, Training loss: 0.06, Train Accuracy: 98.44, Test Accuracy: 81.25\n",
            "Epoch 3.00, Training loss: 0.16, Train Accuracy: 96.88, Test Accuracy: 82.81\n",
            "Epoch 3.03, Training loss: 0.04, Train Accuracy: 98.44, Test Accuracy: 89.06\n",
            "Epoch 3.05, Training loss: 0.10, Train Accuracy: 96.88, Test Accuracy: 81.25\n",
            "Epoch 3.08, Training loss: 0.05, Train Accuracy: 96.88, Test Accuracy: 79.69\n",
            "Epoch 3.10, Training loss: 0.04, Train Accuracy: 98.44, Test Accuracy: 89.06\n",
            "Epoch 3.13, Training loss: 0.05, Train Accuracy: 100.00, Test Accuracy: 81.25\n",
            "Epoch 3.15, Training loss: 0.05, Train Accuracy: 100.00, Test Accuracy: 81.25\n",
            "Epoch 3.18, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 3.21, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 79.69\n",
            "Epoch 3.23, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 78.12\n",
            "Epoch 3.26, Training loss: 0.03, Train Accuracy: 100.00, Test Accuracy: 89.06\n",
            "Epoch 3.28, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 3.31, Training loss: 0.09, Train Accuracy: 96.88, Test Accuracy: 81.25\n",
            "Epoch 3.33, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 3.36, Training loss: 0.23, Train Accuracy: 95.31, Test Accuracy: 87.50\n",
            "Epoch 3.38, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 3.41, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 82.81\n",
            "Epoch 3.44, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 90.62\n",
            "Epoch 3.46, Training loss: 0.11, Train Accuracy: 96.88, Test Accuracy: 84.38\n",
            "Epoch 3.49, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 81.25\n",
            "Epoch 3.51, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 82.81\n",
            "Epoch 3.54, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 3.56, Training loss: 0.13, Train Accuracy: 95.31, Test Accuracy: 82.81\n",
            "Epoch 3.59, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 3.62, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 79.69\n",
            "Epoch 3.64, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 90.62\n",
            "Epoch 3.67, Training loss: 0.11, Train Accuracy: 98.44, Test Accuracy: 89.06\n",
            "Epoch 3.69, Training loss: 0.04, Train Accuracy: 98.44, Test Accuracy: 81.25\n",
            "Epoch 3.72, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 89.06\n",
            "Epoch 3.74, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 3.77, Training loss: 0.04, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 3.79, Training loss: 0.17, Train Accuracy: 95.31, Test Accuracy: 78.12\n",
            "Epoch 3.82, Training loss: 0.08, Train Accuracy: 98.44, Test Accuracy: 87.50\n",
            "Epoch 3.85, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 92.19\n",
            "Epoch 3.87, Training loss: 0.08, Train Accuracy: 96.88, Test Accuracy: 81.25\n",
            "Epoch 3.90, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 78.12\n",
            "Epoch 3.92, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 92.19\n",
            "Epoch 3.95, Training loss: 0.03, Train Accuracy: 100.00, Test Accuracy: 82.81\n",
            "Epoch 3.97, Training loss: 0.10, Train Accuracy: 98.44, Test Accuracy: 84.38\n",
            "Epoch 4.00, Training loss: 0.11, Train Accuracy: 96.88, Test Accuracy: 89.06\n",
            "Epoch 4.03, Training loss: 0.07, Train Accuracy: 98.44, Test Accuracy: 82.81\n",
            "Epoch 4.05, Training loss: 0.03, Train Accuracy: 98.44, Test Accuracy: 90.62\n",
            "Epoch 4.08, Training loss: 0.03, Train Accuracy: 98.44, Test Accuracy: 87.50\n",
            "Epoch 4.10, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 4.13, Training loss: 0.03, Train Accuracy: 98.44, Test Accuracy: 87.50\n",
            "Epoch 4.15, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 4.18, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 89.06\n",
            "Epoch 4.21, Training loss: 0.03, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 4.23, Training loss: 0.03, Train Accuracy: 100.00, Test Accuracy: 89.06\n",
            "Epoch 4.26, Training loss: 0.02, Train Accuracy: 98.44, Test Accuracy: 92.19\n",
            "Epoch 4.28, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 81.25\n",
            "Epoch 4.31, Training loss: 0.05, Train Accuracy: 98.44, Test Accuracy: 89.06\n",
            "Epoch 4.33, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 4.36, Training loss: 0.11, Train Accuracy: 96.88, Test Accuracy: 82.81\n",
            "Epoch 4.38, Training loss: 0.03, Train Accuracy: 98.44, Test Accuracy: 96.88\n",
            "Epoch 4.41, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 76.56\n",
            "Epoch 4.44, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 4.46, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 4.49, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 89.06\n",
            "Epoch 4.51, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 95.31\n",
            "Epoch 4.54, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 4.56, Training loss: 0.13, Train Accuracy: 95.31, Test Accuracy: 82.81\n",
            "Epoch 4.59, Training loss: 0.03, Train Accuracy: 98.44, Test Accuracy: 84.38\n",
            "Epoch 4.62, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 82.81\n",
            "Epoch 4.64, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 4.67, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 4.69, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 76.56\n",
            "Epoch 4.72, Training loss: 0.18, Train Accuracy: 96.88, Test Accuracy: 84.38\n",
            "Epoch 4.74, Training loss: 0.10, Train Accuracy: 98.44, Test Accuracy: 82.81\n",
            "Epoch 4.77, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 92.19\n",
            "Epoch 4.79, Training loss: 0.11, Train Accuracy: 96.88, Test Accuracy: 89.06\n",
            "Epoch 4.82, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 78.12\n",
            "Epoch 4.85, Training loss: 0.10, Train Accuracy: 98.44, Test Accuracy: 84.38\n",
            "Epoch 4.87, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 79.69\n",
            "Epoch 4.90, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 82.81\n",
            "Epoch 4.92, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 90.62\n",
            "Epoch 4.95, Training loss: 0.03, Train Accuracy: 98.44, Test Accuracy: 87.50\n",
            "Epoch 4.97, Training loss: 0.04, Train Accuracy: 98.44, Test Accuracy: 85.94\n",
            "Epoch 5.00, Training loss: 0.09, Train Accuracy: 98.44, Test Accuracy: 82.81\n",
            "Epoch 5.03, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 78.12\n",
            "Epoch 5.05, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 5.08, Training loss: 0.03, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 5.10, Training loss: 0.04, Train Accuracy: 96.88, Test Accuracy: 75.00\n",
            "Epoch 5.13, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 76.56\n",
            "Epoch 5.15, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 5.18, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 5.21, Training loss: 0.00, Train Accuracy: 100.00, Test Accuracy: 90.62\n",
            "Epoch 5.23, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 5.26, Training loss: 0.05, Train Accuracy: 96.88, Test Accuracy: 85.94\n",
            "Epoch 5.28, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 5.31, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 92.19\n",
            "Epoch 5.33, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 82.81\n",
            "Epoch 5.36, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 90.62\n",
            "Epoch 5.38, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 5.41, Training loss: 0.02, Train Accuracy: 98.44, Test Accuracy: 89.06\n",
            "Epoch 5.44, Training loss: 0.18, Train Accuracy: 96.88, Test Accuracy: 76.56\n",
            "Epoch 5.46, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 81.25\n",
            "Epoch 5.49, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 75.00\n",
            "Epoch 5.51, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 5.54, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 79.69\n",
            "Epoch 5.56, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 5.59, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 5.62, Training loss: 0.05, Train Accuracy: 98.44, Test Accuracy: 87.50\n",
            "Epoch 5.64, Training loss: 0.04, Train Accuracy: 98.44, Test Accuracy: 85.94\n",
            "Epoch 5.67, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 81.25\n",
            "Epoch 5.69, Training loss: 0.08, Train Accuracy: 98.44, Test Accuracy: 85.94\n",
            "Epoch 5.72, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 5.74, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 5.77, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 79.69\n",
            "Epoch 5.79, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 76.56\n",
            "Epoch 5.82, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 95.31\n",
            "Epoch 5.85, Training loss: 0.00, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 5.87, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 89.06\n",
            "Epoch 5.90, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 82.81\n",
            "Epoch 5.92, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 81.25\n",
            "Epoch 5.95, Training loss: 0.02, Train Accuracy: 98.44, Test Accuracy: 85.94\n",
            "Epoch 5.97, Training loss: 0.06, Train Accuracy: 96.88, Test Accuracy: 73.44\n",
            "Epoch 6.00, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 6.03, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 6.05, Training loss: 0.00, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 6.08, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 78.12\n",
            "Epoch 6.10, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 81.25\n",
            "Epoch 6.13, Training loss: 0.00, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 6.15, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 79.69\n",
            "Epoch 6.18, Training loss: 0.03, Train Accuracy: 98.44, Test Accuracy: 87.50\n",
            "Epoch 6.21, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 6.23, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 6.26, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 89.06\n",
            "Epoch 6.28, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 81.25\n",
            "Epoch 6.31, Training loss: 0.11, Train Accuracy: 98.44, Test Accuracy: 85.94\n",
            "Epoch 6.33, Training loss: 0.00, Train Accuracy: 100.00, Test Accuracy: 76.56\n",
            "Epoch 6.36, Training loss: 0.00, Train Accuracy: 100.00, Test Accuracy: 75.00\n",
            "Epoch 6.38, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 92.19\n",
            "Epoch 6.41, Training loss: 0.00, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 6.44, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 79.69\n",
            "Epoch 6.46, Training loss: 0.10, Train Accuracy: 98.44, Test Accuracy: 79.69\n",
            "Epoch 6.49, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 89.06\n",
            "Epoch 6.51, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 6.54, Training loss: 0.02, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 6.56, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 82.81\n",
            "Epoch 6.59, Training loss: 0.00, Train Accuracy: 100.00, Test Accuracy: 78.12\n",
            "Epoch 6.62, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 78.12\n",
            "Epoch 6.64, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 82.81\n",
            "Epoch 6.67, Training loss: 0.09, Train Accuracy: 98.44, Test Accuracy: 81.25\n",
            "Epoch 6.69, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 6.72, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 6.74, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 78.12\n",
            "Epoch 6.77, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 87.50\n",
            "Epoch 6.79, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 6.82, Training loss: 0.00, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 6.85, Training loss: 0.01, Train Accuracy: 100.00, Test Accuracy: 85.94\n",
            "Epoch 6.87, Training loss: 0.00, Train Accuracy: 100.00, Test Accuracy: 84.38\n",
            "Epoch 6.90, Training loss: 0.00, Train Accuracy: 100.00, Test Accuracy: 85.94\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-de86eefd0061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_batch\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_train_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mx_test_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_batch\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_test_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_train_batch\u001b[0m \u001b[0;34m}\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmkpxG7Wu9gI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpN4cIcr-IfH",
        "colab_type": "text"
      },
      "source": [
        "##Implement using Keras model\n",
        "The procedure of constructing RNNs and Training can be easily implemented by Keras package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WL_IbPn_Npb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmnnLb_d_wiY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "85ff07ab-4caf-4332-be71-54d3eb231607"
      },
      "source": [
        "embedding_size = 8\n",
        "\n",
        "# contruct RNNs\n",
        "model = Sequential()\n",
        "model.add( Embedding( input_dim =vocabulary_size, output_dim= embedding_size, input_length = sequence_len, name = \"layer_embedding\" )  )\n",
        "model.add( GRU( units = 16, return_sequences = True  ) )\n",
        "model.add( GRU( units = 8, return_sequences = True ) )\n",
        "model.add( GRU( units = 4, return_sequences = False  ) )\n",
        "model.add( Dense( 1, activation=\"sigmoid\" ) )\n",
        "\n",
        "# config training and testing\n",
        "model.compile(  loss = \"binary_crossentropy\", optimizer = Adam(lr=1e-3), metrics = [\"accuracy\"] )\n",
        "model.summary() ## not necessary"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_embedding (Embedding)  (None, 579, 8)            708672    \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 579, 16)           1200      \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 579, 8)            600       \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, 4)                 156       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 710,633\n",
            "Trainable params: 710,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfJCrx70BvJ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "4cbc2be7-6481-4d12-fc70-056a35beca3d"
      },
      "source": [
        "model.fit(  x_train_pad, y_train, validation_split=0.05, epochs=3, batch_size= 64   )"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 23750 samples, validate on 1250 samples\n",
            "Epoch 1/3\n",
            "23750/23750 [==============================] - 913s 38ms/sample - loss: 0.4757 - acc: 0.7488 - val_loss: 0.3702 - val_acc: 0.8352\n",
            "Epoch 2/3\n",
            "23750/23750 [==============================] - 904s 38ms/sample - loss: 0.2289 - acc: 0.9161 - val_loss: 0.2953 - val_acc: 0.8792\n",
            "Epoch 3/3\n",
            "23750/23750 [==============================] - 907s 38ms/sample - loss: 0.1310 - acc: 0.9583 - val_loss: 0.4771 - val_acc: 0.8168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa68d43a780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBP4LyBqBsad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "35f4cf0e-af27-4369-8a04-e0fe8561e910"
      },
      "source": [
        "## model.evaluate(x,y)  # evaluate on the test dataset using the provided metrics \n",
        "## model.predict(x)  # predict a batch of x given a batch of y \n",
        "results = model.evaluate( x_test_pad[:2000], y_test[:2000] )"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 58s 29ms/sample - loss: 0.2574 - acc: 0.8985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bUJnFR5ZOWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c1f941df-9497-4d14-cee8-b853ed9f3eb8"
      },
      "source": [
        "results"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2573547387123108, 0.8985]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9o8Yo2gZx0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "b2633b6c-dd82-4172-d01f-33eda4d76c25"
      },
      "source": [
        "example_x=[\n",
        "    \n",
        "    \"This film is the worst one I have ever seen!\",\n",
        "    \"I cannot forget how many people are laughing when the actors are dancing\",\n",
        "    \"Mediocre!\",\n",
        "    \"Greatest ever!\",\n",
        "    \"Wow what a great surprise this was. I was told by a friend this was good but it\\'s been awhile since I liked a Keanu movie so I was hesitant to try it. Retired hit-man John Wick (Keanu Reeves) loses his wife to cancer. After her funeral he receives a puppy she left him. A few days later some thugs, led by the son of a Russian gangster John used to work for, break into John's house. They beat him up, take the keys to his beloved car, and kill the puppy. They did this not knowing who he was; they just wanted the car. Now John Wick is out for revenge and the Russian gangster is trying to save his son's life by sending killers after John. Keanu\\'s great here. Glad to see him doing something watchable again. Willem Dafoe, Alfie Allen, Ian McShane, and Lance Reddick lead a good supporting cast. Michael Nyqvist was made to play villains. Even Adrianne Palicki was good. Oh and hey the beat-up guy from the Allstate commercials is in this. The stuff with the hotel for assassins and the way they all know each other was pretty funny. About the only problem I had with it was the unrealistic scene where the bad guy finally gets the upper hand on the 'hero' and doesn't kill him. This sort of thing is common in movies but it's always unbelievable and reminds me of the old James Bond villains. This is easily the best action movie this year. Possibly the best straight action movie since the first Taken. English-speaking action movies, that is. It doesn't reinvent the genre or anything but it's entertaining.\",\n",
        "    ]\n",
        "\n",
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.word_index = word_index\n",
        "example_x_tokens = my_tokenizer.texts_to_sequences( example_x )\n",
        "example_x_tokens_pad = pad_sequences( example_x_tokens, maxlen = sequence_len, padding = pad, truncating= pad  )\n",
        "\n",
        "example_pred_y = model.predict(example_x_tokens_pad)\n",
        "print(example_pred_y)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.08563638]\n",
            " [0.5451344 ]\n",
            " [0.64223486]\n",
            " [0.9524087 ]\n",
            " [0.9731012 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjOCxdJ_tD9B",
        "colab_type": "text"
      },
      "source": [
        "#### Analysis of the word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVfLKgmNbklr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64b5c5ee-877a-466f-a526-aad47d1fadef"
      },
      "source": [
        "embedding_layer = model.get_layer(\"layer_embedding\")\n",
        "word_embeddings = embedding_layer.get_weights()[0]  # there could be multiple weight matrixs\n",
        "\n",
        "print(word_embeddings.shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(88584, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9AWJit0wpn_",
        "colab_type": "text"
      },
      "source": [
        "We can check the vocabulary size and value of tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnQglhdiwpVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "11b53c64-3625-45f0-b003-db1328f5c328"
      },
      "source": [
        "token_list =[]\n",
        "for key in word_index.keys():\n",
        "  token_list.append(word_index[key])\n",
        "\n",
        "print(min(token_list))\n",
        "print(max(token_list))\n",
        "print(len(token_list))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "88584\n",
            "88584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfrP4gTrwd5r",
        "colab_type": "text"
      },
      "source": [
        "So word embeddings is a ndarray of the shape [ vocabulary_size, embedding_dimension ] (look-up matrix). We also notice that in the word_index the tokens are consecutively from 1 to vocabulary_size. Therefore, if we have a word \"good\" with a token=100, then the word embedding of \"good\" should be word_embeddings[100-1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE8snjwStWQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "44edde4b-37fb-4472-b79f-02672a4b00bc"
      },
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def parse_distance_of_word(word, k=10):\n",
        "  \n",
        "  embedding_of_current_word = word_embeddings[ word_index[word] -1]\n",
        "  distances = cdist( word_embeddings, [embedding_of_current_word], metric = \"cosine\"  )[:,0].T.tolist()\n",
        "  token_index = (np.arange( len(distances) )+1).tolist()\n",
        "  sorted_token_index = [x for _,x in sorted( zip( distances, token_index ), key= lambda pair: pair[0]  )  ]\n",
        "  sorted_word_list = [inverse_word_index[ids] for ids in sorted_token_index ]\n",
        "  \n",
        "  print(\"The %d closest words:\"%(k)  )\n",
        "  print(sorted_word_list[:k])\n",
        "  \n",
        "  print(\"The %d most different words:\"%(k)  )\n",
        "  print(sorted_word_list[-k:])\n",
        "  \n",
        "\n",
        "parse_distance_of_word(\"great\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 10 closest words:\n",
            "['great', 'fillums', \"margheriti's\", 'intimacies', 'onion', 'helumis', 'nandani', 'colada', 'farfella', 'testosterone']\n",
            "The 10 most different words:\n",
            "['subsequences', 'jaimie', '0000000000001', 'suoi', \"'acting\", 'bunch', 'forgery', \"'futurama'\", 'jarada', 'frescorts']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}