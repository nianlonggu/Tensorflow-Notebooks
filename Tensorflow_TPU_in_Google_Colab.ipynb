{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow TPU in Google Colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nianlonggu/Tensorflow-Notebooks/blob/master/Tensorflow_TPU_in_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9bd0Zy5SZGMz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A simple tutorial of using Tensorflow TPU in Google Colab"
      ]
    },
    {
      "metadata": {
        "id": "iciYlDFxZbLT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this document, I provide a full example of how to use tensorfloe TPU in Colab. The common example of iris species recognition provided in [official turtorials](https://cloud.google.com/tpu/docs/tutorials/migrating-to-tpuestimator-api) contains many extra information, like dealing with the feature columns and manage the name of each dimension of the feature vector. This makes it sometimes misleading for the readers  to capture the key aspect of how to use TPU in their customized code. In this example, I modified the official document, and try to achieve the task of hand-written digit recognition in MNIST dataset.\n",
        "\n",
        "The main topics include: \n",
        "1. **tensorflow Dataset API**\n",
        "2. **tensorflow TPUEstimator API**\n",
        "3. **Google Cloud Storage**"
      ]
    },
    {
      "metadata": {
        "id": "t_wF_CiVcTll",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task Description\n",
        "Given an hand-written image, like: \n",
        "\n",
        "![MNIST](https://github.com/nianlonggu/Tensorflow-Notebooks/blob/master/figures/5.png?raw=true)\n",
        "\n",
        "the task is to train a CNN to correctly detect the number of the hand-written digit (5 in this case).  A typical way for training is to train the model using large number of ( **X**, **y** ) pairs, where **X** is the pixel matrix of an image and **y** is the corresponding label. This is a fully supervised training scheme.\n",
        "\n",
        "To use TPU tp achive this task, there are multiple ways. One is first design the model using keras, and then use the ** tf.contrib.tpu.keras_to_tpu_model** to cast the code to TPU specific code. Clike[ here ](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb)to see the introduction.\n",
        "\n",
        "Another way is to design a customized TPUEstimator, and then run the train, evaluate and predict function. This method has a unified framework for general supervised training problems and is easier to tranfer over different computation platforms (CPU, GPU, TPU). This is also the main topic of this document."
      ]
    },
    {
      "metadata": {
        "id": "G7u7Ffe1iWQx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Start of Coding"
      ]
    },
    {
      "metadata": {
        "id": "AXWiGxYqjZIp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### import python libraries\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "M-I2EXgTDknv",
        "colab_type": "code",
        "outputId": "c7314d8a-81b9-45c2-c147-ccb40734b6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras.datasets.mnist as mnist\n",
        "import math\n",
        "import os\n",
        "import tempfile\n",
        "tf.enable_eager_execution()\n",
        "flags = tf.app.flags \n",
        "FLAGS = tf.app.flags.FLAGS\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "uLnANUUFYY65",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y4Cp8KCMy3Mb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aFuUNy-m_j40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "## TPU Cluster Resolver flags\n",
        "flags.DEFINE_string(\"tpu\", default= 'grpc://' + os.environ['COLAB_TPU_ADDR'], help= \"used for tpu resovler, containing the address of TPUs\" )\n",
        "\n",
        "## Model specific parameters\n",
        "flags.DEFINE_string(\"model_dir\", \"gs://carlos-vic.appspot.com\", \"define the path to store the checkpoint files\"  )\n",
        "flags.DEFINE_integer('batch_size', 128, 'batch_size is usually 2^n')\n",
        "flags.DEFINE_integer('train_steps', 2000, 'total number of train steps')\n",
        "# WHat's this?\n",
        "flags.DEFINE_integer('eval_steps', 4, 'total number of eval steps, skipped if 0')\n",
        "\n",
        "## TPU specific parameters\n",
        "flags.DEFINE_bool( 'use_tpu', True, 'True for using TPU' )\n",
        "# What's this?\n",
        "flags.DEFINE_integer('iterations', 500, 'number of iterations per TPU training loop' )  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cEj29qwUdpW1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# FLAGS.model_dir = \".\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fc08DtL5G-CZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preprocessing the data and make them into the format of ndarray; But there could be other format of data which is also supported by tf.contrib.estimator"
      ]
    },
    {
      "metadata": {
        "id": "40WWghkOD5bN",
        "colab_type": "code",
        "outputId": "0c2ca579-0d33-471a-b061-fbc8c40b91b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_val = x_train[-5000:]/255.0\n",
        "x_val = np.expand_dims(x_val, -1).astype(np.float32)\n",
        "y_val = y_train[-5000:].astype(np.int32)\n",
        "x_train = x_train[ :-5000]/255.0\n",
        "x_train = np.expand_dims(x_train, -1).astype(np.float32)\n",
        "y_train = y_train[ :-5000].astype(np.int32)  ## by default y_train is not one-hot coded\n",
        "x_test = x_test/255.0\n",
        "x_test = np.expand_dims(x_test, -1).astype(np.float32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "\n",
        "print( x_val.shape )\n",
        "print( x_train.shape )\n",
        "print( x_test.shape )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 28, 28, 1)\n",
            "(55000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hY9f0XauHYX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define some global variables "
      ]
    },
    {
      "metadata": {
        "id": "QOJ9YaI7KcDh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the helper function for plotting images."
      ]
    },
    {
      "metadata": {
        "id": "nYo69jSGGFP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_images( images, labels=None, preds= None  ):\n",
        "  images = np.squeeze(images )\n",
        "  num = images.shape[0]\n",
        "  n_rows = math.floor( math.sqrt( num ))\n",
        "  n_columns = math.ceil(math.sqrt(num))\n",
        "\n",
        "  fig, axes = plt.subplots( n_rows, n_columns )\n",
        "  \n",
        "  if n_rows * n_columns == 1:\n",
        "    axes = [axes]\n",
        "  else:\n",
        "    axes = axes.flat\n",
        "  \n",
        "  for i, ax in enumerate( axes ):\n",
        "    if i < num:\n",
        "      ax.imshow( images[i] )\n",
        "      xlabel= \"\"\n",
        "      if labels is not None:\n",
        "        xlabel += \"True: %d \"%(labels[i])\n",
        "      if preds is not None:\n",
        "        xlabel += \"Pred: %d\"%(preds[i])\n",
        "      ax.set_xlabel(xlabel)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "411qovFhy-EF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Define the function for inputing data to the Estimator"
      ]
    },
    {
      "metadata": {
        "id": "jr9PECWWm0Eu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_input_fn( features, labels, batch_size ):\n",
        "\n",
        "    \"\"\"An input function for training\"\"\"\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    ## shuffle repeat and batch\n",
        "    # Shuffle, repeat.\n",
        "    dataset = dataset.shuffle(1000).repeat()\n",
        "    # TPU specific batch the slices, to make sure that each batch size is divisible by the number of TPU cores\n",
        "    dataset = dataset.batch( batch_size, drop_remainder = True )\n",
        "    # Return the dataset.\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G7gKO1E5Q-N3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval_input_fn(features, labels, batch_size):\n",
        "    \"\"\"An input function for training\"\"\"\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    dataset = dataset.shuffle(1000).repeat()\n",
        "    dataset = dataset.batch(batch_size, drop_remainder = True )\n",
        "    # Return the dataset.\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q6pCeeWMTBp7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_input_fn(features, labels,batch_size):\n",
        "    \"\"\"An input function for training\"\"\"\n",
        "    # Convert the inputs to a Dataset.\n",
        "    # For predict use_tpu should be False, since drop_remainder is False\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    # Return the dataset.\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoupeiiFo1_N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def metric_fn(labels, logits):\n",
        "    \"\"\"Function to return metrics for evaluation.\"\"\"\n",
        "\n",
        "    predicted_classes = tf.argmax(logits, 1)\n",
        "    accuracy = tf.metrics.accuracy(labels=labels,\n",
        "                                   predictions=predicted_classes,\n",
        "                                   name=\"acc_op\")\n",
        "    return {\"accuracy\": accuracy}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HiewctTEVQsf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Self Defined Estimator"
      ]
    },
    {
      "metadata": {
        "id": "TDnPK26tVW-N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # Args:\n",
        "    #\n",
        "    # features: This is the x-arg from the input_fn.\n",
        "    # labels:   This is the y-arg from the input_fn,\n",
        "    #           see e.g. train_input_fn for these two.\n",
        "    # mode:     Either TRAIN, EVAL, or PREDICT\n",
        "    # params:   User-defined hyper-parameters, e.g. learning-rate.\n",
        "    \n",
        "    # Reference to the tensor named \"x\" in the input-function.\n",
        "#     x = features[\"x\"]\n",
        "\n",
        "    ## create the model networks\n",
        "    x = features\n",
        "    # First convolutional layer.\n",
        "    net = tf.layers.conv2d(inputs=x, name='layer_conv1',\n",
        "                           filters=16, kernel_size=5,\n",
        "                           padding='same', activation=tf.nn.relu)\n",
        "    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n",
        "\n",
        "    # Second convolutional layer.\n",
        "    net = tf.layers.conv2d(inputs=net, name='layer_conv2',\n",
        "                           filters=36, kernel_size=5,\n",
        "                           padding='same', activation=tf.nn.relu)\n",
        "    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)    \n",
        "\n",
        "    # Flatten to a 2-rank tensor.\n",
        "    net = tf.contrib.layers.flatten(net)\n",
        "    # Eventually this should be replaced with:\n",
        "    # net = tf.layers.flatten(net)\n",
        "\n",
        "    # First fully-connected / dense layer.\n",
        "    # This uses the ReLU activation function.\n",
        "    net = tf.layers.dense(inputs=net, name='layer_fc1',\n",
        "                          units=128, activation=tf.nn.relu)    \n",
        "\n",
        "    # Second fully-connected / dense layer.\n",
        "    # This is the last layer so it does not use an activation function.\n",
        "    net = tf.layers.dense(inputs=net, name='layer_fc2',units=10)\n",
        "\n",
        "    ## compute logits\n",
        "    # Logits output of the neural network.\n",
        "    logits = net\n",
        "    \n",
        "    ## for training and evaluation\n",
        "    ## compute loss\n",
        "    loss =  tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
        "                                                                       logits=logits))\n",
        "    # Softmax output of the neural network.\n",
        "    y_pred_probabilities = tf.nn.softmax(logits=logits)\n",
        "    # Classification output of the neural network.\n",
        "    y_pred_classes = tf.argmax(y_pred_probabilities, axis=1)[:, tf.newaxis]\n",
        "    # Prediction Accuracy for evaluation\n",
        "    pred_accuracy = tf.metrics.accuracy( labels = labels, predictions = y_pred_classes , name = \"op_acc\" )\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        predictions = {\n",
        "              \"logits\": logits,\n",
        "              \"class_ids\": y_pred_classes,\n",
        "              \"probabilities\": y_pred_probabilities,\n",
        "        }\n",
        "        \n",
        "        spec = tf.contrib.tpu.TPUEstimatorSpec(mode=mode, predictions=predictions)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "        spec = tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=loss, eval_metrics=(metric_fn, [labels, logits] ) )\n",
        "\n",
        "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
        "  \n",
        "        # Define the optimizer for improving the neural network.\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\n",
        "        \n",
        "        if FLAGS.use_tpu:\n",
        "            optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "        train_op = optimizer.minimize(\n",
        "            loss=loss, global_step=tf.train.get_global_step())\n",
        "\n",
        "#         # Define the evaluation metrics,\n",
        "#         # in this case the classification accuracy.\n",
        "#         metrics = \\\n",
        "#         {\n",
        "#             \"accuracy\": tf.metrics.accuracy(labels, y_pred_cls)\n",
        "#         }\n",
        "\n",
        "#         # Wrap all of this in an EstimatorSpec.\n",
        "# #         spec = tf.estimator.EstimatorSpec(\n",
        "# #             mode=mode,\n",
        "# #             loss=loss,\n",
        "# #             train_op=train_op,\n",
        "# #             eval_metric_ops=metrics)\n",
        "        \n",
        "        spec= tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=loss,train_op= train_op, eval_metrics=(metric_fn, [labels, logits] ) )\n",
        "        \n",
        "    return spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iQEvS-dadw0U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "    FLAGS.tpu)\n",
        "\n",
        "# tpu_config = os.environ.get('TF_CONFIG')\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    model_dir=FLAGS.model_dir,\n",
        "    cluster=tpu_cluster_resolver,\n",
        "#     save_checkpoints_steps = 0,\n",
        "    session_config=tf.ConfigProto(\n",
        "        allow_soft_placement=True, log_device_placement=True),\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(FLAGS.iterations),\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ovokb2YRWPG8",
        "colab_type": "code",
        "outputId": "2e95824a-929b-4418-d7f9-037b276e0132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "cell_type": "code",
      "source": [
        "# model = tf.estimator.Estimator(model_fn=model_fn,\n",
        "#                                params={\"learning_rate\": 1e-4},\n",
        "#                                model_dir=\"./checkpoints_tutorial17-2/\")\n",
        "\n",
        "model = tf.contrib.tpu.TPUEstimator(\n",
        "                               model_fn=model_fn,\n",
        "                               params = {\"learning_rate\": 1e-4 },\n",
        "                               config = run_config,\n",
        "                               use_tpu= FLAGS.use_tpu,\n",
        "                               train_batch_size=FLAGS.batch_size,\n",
        "                               eval_batch_size=FLAGS.batch_size,\n",
        "                               predict_batch_size=FLAGS.batch_size,\n",
        "  \n",
        "                               \n",
        "                                )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://carlos-vic.appspot.com', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "log_device_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.11.28.90:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb18704b00>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.11.28.90:8470', '_evaluation_master': 'grpc://10.11.28.90:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fcb18a0e860>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "odBUseLSWYtl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "2ON1-FUS7VoX",
        "colab_type": "code",
        "outputId": "cfcd3bb3-2f2d-455b-be1d-13c41cf777fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "!gsutil mb gs://carlos-vic.appspot.com"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating gs://carlos-vic.appspot.com/...\n",
            "You are attempting to perform an operation that requires a project id, with none configured. Please re-run gsutil config and make sure to follow the instructions for finding and entering your default project id.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g9pPDsivykHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t8dY5URVWa5x",
        "colab_type": "code",
        "outputId": "c0a1cdd6-abef-4f4b-89d7-5d41ad912205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1019
        }
      },
      "cell_type": "code",
      "source": [
        "model.train( input_fn = lambda params: train_input_fn( x_train, y_train, params[\"batch_size\"] ), steps=2000  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://carlos-vic.appspot.com/model.ckpt-100000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 100000 into gs://carlos-vic.appspot.com/model.ckpt.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 7 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 2.8953691e-05, step = 100500\n",
            "INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 4.0757136e-06, step = 101000 (0.756 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.996\n",
            "INFO:tensorflow:examples/sec: 84607.5\n",
            "INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.0, step = 101500 (0.570 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.715\n",
            "INFO:tensorflow:examples/sec: 112220\n",
            "INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.0, step = 102000 (0.546 sec)\n",
            "INFO:tensorflow:global_step/sec: 916.336\n",
            "INFO:tensorflow:examples/sec: 117291\n",
            "INFO:tensorflow:Saving checkpoints for 102000 into gs://carlos-vic.appspot.com/model.ckpt.\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Loss for final step: 0.0.\n",
            "INFO:tensorflow:training_loop marked as finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.contrib.tpu.python.tpu.tpu_estimator.TPUEstimator at 0x7fcb20e39f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "2-TdmDsUtAN8",
        "colab_type": "code",
        "outputId": "a35340ca-b7b9-4e7d-b858-23a590aeb717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        }
      },
      "cell_type": "code",
      "source": [
        "eval_result = model.evaluate(\n",
        "      input_fn=lambda params: eval_input_fn(\n",
        "          x_val, y_val, params[\"batch_size\"]),\n",
        "      steps=FLAGS.eval_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-04-08T12:43:37Z\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://carlos-vic.appspot.com/model.ckpt-102000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 9 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (4) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (4) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Evaluation [4/4]\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Finished evaluation at 2019-04-08-12:43:49\n",
            "INFO:tensorflow:Saving dict for global step 102000: accuracy = 0.9902344, global_step = 102000, loss = 8.535862e-05\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 102000: gs://carlos-vic.appspot.com/model.ckpt-102000\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oo_STluG0nVU",
        "colab_type": "code",
        "outputId": "2d4b3f8a-93f0-421c-a84a-fe79604feb99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "predict_result = model.predict(\n",
        "      input_fn=lambda params: predict_input_fn(\n",
        "          x_test,y_test, params[\"batch_size\"]))\n",
        "\n",
        "\n",
        "next(predict_result)\n",
        "\n",
        "# for pred_dict, expec in zip( predict_result, y_test  ):\n",
        "#   class_id = pred_dict[\"class_ids\"][0]\n",
        "#   prob_pred = pred_dict[\"probabilities\"][class_id]\n",
        "#   print( \"Prediction is %d, probility %.1f, expected %d\" %( class_id, prob_pred, expec )  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://carlos-vic.appspot.com/model.ckpt-102000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 7 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class_ids': array([7]),\n",
              " 'logits': array([-18.951256  ,   1.2203839 ,  -0.69914997,   2.895075  ,\n",
              "        -35.774868  , -12.417238  , -79.63498   ,  50.539818  ,\n",
              "          4.910502  ,   6.9981675 ], dtype=float32),\n",
              " 'probabilities': array([6.6131789e-31, 3.8092727e-22, 5.5872563e-23, 2.0330702e-21,\n",
              "        3.2659478e-38, 4.5509267e-28, 0.0000000e+00, 1.0000000e+00,\n",
              "        1.5256014e-20, 1.2305580e-19], dtype=float32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "aEotpUEtP4_A",
        "colab_type": "code",
        "outputId": "2f29bca5-e2bb-489a-9966-16cad7cf922a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "next(predict_result, 10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class_ids': array([5]),\n",
              " 'logits': array([-24.94518 , -49.249718, -20.357456, -28.440752, -11.425112,\n",
              "         35.63763 ,  17.286167, -20.439623,  10.581571, -12.081831],\n",
              "       dtype=float32),\n",
              " 'probabilities': array([4.88899473e-27, 1.36111573e-37, 4.80442570e-25, 1.48289985e-28,\n",
              "        3.63839229e-21, 1.00000000e+00, 1.07167075e-08, 4.42545347e-25,\n",
              "        1.31307925e-11, 1.88669861e-21], dtype=float32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "K4h4esNSuMEU",
        "colab_type": "code",
        "outputId": "fa732440-4434-4937-8243-3c593f3f4b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "predict_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object TPUEstimator.predict at 0x7f05c0106990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "k80CFD898yIK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#  you may not use this file except in compliance with the License.\n",
        "#  You may obtain a copy of the License at\n",
        "#\n",
        "#   http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#  Unless required by applicable law or agreed to in writing, software\n",
        "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#  See the License for the specific language governing permissions and\n",
        "#  limitations under the License.\n",
        "\n",
        "\"\"\"Module to generate Iris dataset for using in custom TPUEstimator.\"\"\"\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "TRAIN_URL = 'http://download.tensorflow.org/data/iris_training.csv'\n",
        "TEST_URL = 'http://download.tensorflow.org/data/iris_test.csv'\n",
        "\n",
        "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
        "                    'PetalLength', 'PetalWidth', 'Species']\n",
        "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
        "\n",
        "PREDICTION_INPUT_DATA = {\n",
        "    'SepalLength': [6.9, 5.1, 5.9],\n",
        "    'SepalWidth': [3.1, 3.3, 3.0],\n",
        "    'PetalLength': [5.4, 1.7, 4.2],\n",
        "    'PetalWidth': [2.1, 0.5, 1.5],\n",
        "}\n",
        "\n",
        "PREDICTION_OUTPUT_DATA = ['Virginica', 'Setosa', 'Versicolor']\n",
        "\n",
        "\n",
        "def maybe_download():\n",
        "  train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\n",
        "  test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
        "\n",
        "  return train_path, test_path\n",
        "\n",
        "\n",
        "def load_data(y_name='Species'):\n",
        "  \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\n",
        "  train_path, test_path = maybe_download()\n",
        "\n",
        "  train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0,\n",
        "                      dtype={'SepalLength': pd.np.float32,\n",
        "                             'SepalWidth': pd.np.float32,\n",
        "                             'PetalLength': pd.np.float32,\n",
        "                             'PetalWidth': pd.np.float32,\n",
        "                             'Species': pd.np.int32})\n",
        "  train_x, train_y = train, train.pop(y_name)\n",
        "\n",
        "  test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0,\n",
        "                     dtype={'SepalLength': pd.np.float32,\n",
        "                            'SepalWidth': pd.np.float32,\n",
        "                            'PetalLength': pd.np.float32,\n",
        "                            'PetalWidth': pd.np.float32,\n",
        "                            'Species': pd.np.int32})\n",
        "  test_x, test_y = test, test.pop(y_name)\n",
        "\n",
        "  return (train_x, train_y), (test_x, test_y)\n",
        "\n",
        "\n",
        "def train_input_fn(features, labels, batch_size):\n",
        "  \"\"\"An input function for training.\"\"\"\n",
        "\n",
        "  # Convert the inputs to a Dataset.\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "  # Shuffle, repeat, and batch the examples.\n",
        "  dataset = dataset.shuffle(1000).repeat()\n",
        "\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  # Return the dataset.\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def eval_input_fn(features, labels, batch_size):\n",
        "  \"\"\"An input function for evaluation.\"\"\"\n",
        "  features = dict(features)\n",
        "  inputs = (features, labels)\n",
        "\n",
        "  # Convert the inputs to a Dataset.\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "  dataset = dataset.shuffle(1000).repeat()\n",
        "\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  # Return the dataset.\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def predict_input_fn(features, batch_size):\n",
        "  \"\"\"An input function for prediction.\"\"\"\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(features)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j06wysEPVXzX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Prediction"
      ]
    },
    {
      "metadata": {
        "id": "FSQN1XAkC6I3",
        "colab_type": "code",
        "outputId": "aba8bb37-5bb8-4dd2-98bd-4e05e3bfd36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "test_results = model.evaluate( input_fn = test_input_fn )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2abcefce3cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_input_fn\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_input_fn' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "roHQ3DkTQo6G",
        "colab_type": "code",
        "outputId": "1df12d8b-678c-44d1-acfd-e8a3e6aec961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "test_results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.977,\n",
              " 'average_loss': 0.083602235,\n",
              " 'global_step': 2000,\n",
              " 'loss': 10.5825615}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "metadata": {
        "id": "ER_E-9mUQrUZ",
        "colab_type": "code",
        "outputId": "d8ead1f4-4671-4569-e3a2-c1c9b2d4dcd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Classification accuracy: %.2f%%\"%(test_results[\"accuracy\"] * 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification accuracy: 97.70%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FD8NQGeWRJ7e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}