{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow TPU in Google Colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nianlonggu/Tensorflow-Notebooks/blob/master/Tensorflow_TPU_in_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9bd0Zy5SZGMz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A simple tutorial of using Tensorflow TPU in Google Colab"
      ]
    },
    {
      "metadata": {
        "id": "iciYlDFxZbLT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this document, I provide a full example of how to use tensorfloe TPU in Colab. The common example of iris species recognition provided in [official turtorials](https://cloud.google.com/tpu/docs/tutorials/migrating-to-tpuestimator-api) contains many extra information, like dealing with the feature columns and manage the name of each dimension of the feature vector. This makes it sometimes misleading for the readers  to capture the key aspect of how to use TPU in their customized code. In this example, I modified the official document, and try to achieve the task of hand-written digit recognition in MNIST dataset.\n",
        "\n",
        "The main topics include: \n",
        "1. **tensorflow Dataset API**\n",
        "2. **tensorflow TPUEstimator API**\n",
        "3. **Google Cloud Storage**"
      ]
    },
    {
      "metadata": {
        "id": "t_wF_CiVcTll",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task Description\n",
        "Given an hand-written image, like: \n",
        "\n",
        "![MNIST](https://github.com/nianlonggu/Tensorflow-Notebooks/blob/master/figures/5.png?raw=true)\n",
        "\n",
        "the task is to train a CNN to correctly detect the number of the hand-written digit (5 in this case).  A typical way for training is to train the model using large number of ( **X**, **y** ) pairs, where **X** is the pixel matrix of an image and **y** is the corresponding label. This is a fully supervised training scheme.\n",
        "\n",
        "To use TPU tp achive this task, there are multiple ways. One is first design the model using keras, and then use the ** tf.contrib.tpu.keras_to_tpu_model** to cast the code to TPU specific code. Clike[ here ](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb)to see the introduction.\n",
        "\n",
        "Another way is to design a customized TPUEstimator, and then run the train, evaluate and predict function. This method has a unified framework for general supervised training problems and is easier to tranfer over different computation platforms (CPU, GPU, TPU). This is also the main topic of this document.\n",
        "\n",
        "**Note**: before runing, clike in colab the \"Edit\"->\"Notebook Setting\" and choose TPU as accelerator!"
      ]
    },
    {
      "metadata": {
        "id": "G7u7Ffe1iWQx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Start of Coding"
      ]
    },
    {
      "metadata": {
        "id": "AXWiGxYqjZIp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### import python libraries"
      ]
    },
    {
      "metadata": {
        "id": "M-I2EXgTDknv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras.datasets.mnist as mnist\n",
        "import math\n",
        "import os\n",
        "## plt is used to plot the results\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OF0U1CV5mi9J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Different from keras_to_tpu_model, if we want use TPUEstimator in colab, the checkpoints files cannot be stored locally in colab. You will get an error like:\n",
        "\n",
        "***file system scheme '[local]' not implemented***\n",
        "\n",
        "As far as I know, the only choice is to store the ckpt files in **Google Cloud Storage**.  To solve this without any additional cost, you can visit https://cloud.google.com/storage/ , apply for a 1-year free trail accout, and activate the billing service. After this, following the website instruction to create a new Google cloud storage bucket with a name like \"awesome_gcs_bucket\". Then you get a address to acess the GCS: ** gs://awesome_gcs_bucket**  \n",
        "\n",
        "After get the GCS address, you need to authorize you google colab to get access to you Google Cloud Storage Busket using the following commands:"
      ]
    },
    {
      "metadata": {
        "id": "Y4Cp8KCMy3Mb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "043b9612-1dee-4257-a4aa-2cb07ef9237a"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NH5_NWmzslGq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a Flag Class to hold all the global hyperparameters. \n",
        "This can also be achived by using tf.flags. Note that there are some bugs realated with tf.flags in Colab, simply add \"flags.DEFINE_string('f', '', 'kernel')\" to avoid it"
      ]
    },
    {
      "metadata": {
        "id": "RsoYNtDeuBvj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Flags:\n",
        "  def __init__(self):\n",
        "    ## tpu store the address of TPU, used to passed to the TPUClusterResolver\n",
        "    self.tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    ## model_dir specify the path to store the model parameters. Here we need to use the adress to the cloud storage bucket\n",
        "    ## model_dir will be passed to the tpu.RunConfig() function\n",
        "    self.model_dir = \"gs://awesome_gcs_bucket\"\n",
        "    \"\"\" batch_size: the batch size used for training, evaluation and prediction. It should be a multiple of the number of TPU cores\n",
        "    Normally the number of TPU cores is 8, so the batch_size is usually 128. The reason for this is during TPU running, each batch of \n",
        "    data will be dispatched to each TPU cores, so the batch size should be divisable by the number of TPU cores.\"\"\"\n",
        "    self.batch_size = 128\n",
        "    \"\"\" train_steps define the number of steps of the total training procedure. In each step a batch of data a fed into the TPU for \n",
        "    computation. Note that in TPUEstimator train function there are two parameters: steps and max_steps. \"steps\" means the further number of steps to be trained from now;\n",
        "    \"max_steps\" means the total maximum number of steps.\n",
        "    \"\"\"\n",
        "    self.train_steps = 10000\n",
        "    ## iterations\n",
        "    \n",
        "    ## the bool flag for using tpu or not\n",
        "    self.use_tpu = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t9kzXWYnuUha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "371c3d73-866d-4743-b490-7ea6016c18c6"
      },
      "cell_type": "code",
      "source": [
        "FLAGS = Flags()\n",
        "FLAGS.tpu"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'grpc://10.103.181.170:8470'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "aFuUNy-m_j40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "outputId": "c34083f5-3ae1-467b-f51d-b47026d77721"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# gs://awesome_gcs_bucket\n",
        "from tensorflow import flags\n",
        "\n",
        "# flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "## TPU Cluster Resolver flags\n",
        "flags.DEFINE_string(\"tpu\", default= 'grpc://' + os.environ['COLAB_TPU_ADDR'], help= \"used for tpu resovler, containing the address of TPUs\" )\n",
        "\n",
        "## Model specific parameters\n",
        "flags.DEFINE_string(\"model_dir\", \"gs://carlos-vic.appspot.com\", \"define the path to store the checkpoint files\"  )\n",
        "flags.DEFINE_integer('batch_size', 128, 'batch_size is usually 2^n')\n",
        "flags.DEFINE_integer('train_steps', 2000, 'total number of train steps')\n",
        "# WHat's this?\n",
        "flags.DEFINE_integer('eval_steps', 4, 'total number of eval steps, skipped if 0')\n",
        "\n",
        "## TPU specific parameters\n",
        "flags.DEFINE_bool( 'use_tpu', True, 'True for using TPU' )\n",
        "# What's this?\n",
        "flags.DEFINE_integer('iterations', 500, 'number of iterations per TPU training loop' )  \n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DuplicateFlagError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-929b244d6f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## TPU Cluster Resolver flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'grpc://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"used for tpu resovler, containing the address of TPUs\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m## Model specific parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_string\u001b[0;34m(name, default, help, flag_values, **args)\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m   \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \"\"\"\n\u001b[1;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[0;32m---> 82\u001b[0;31m               flag_values, module_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'tpu' is defined twice. First from /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py, Second from /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py.  Description from first occurrence: used for tpu resovler, containing the address of TPUs"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cEj29qwUdpW1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# FLAGS.model_dir = \".\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fc08DtL5G-CZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preprocessing the data and make them into the format of ndarray; But there could be other format of data which is also supported by tf.contrib.estimator"
      ]
    },
    {
      "metadata": {
        "id": "40WWghkOD5bN",
        "colab_type": "code",
        "outputId": "0c2ca579-0d33-471a-b061-fbc8c40b91b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_val = x_train[-5000:]/255.0\n",
        "x_val = np.expand_dims(x_val, -1).astype(np.float32)\n",
        "y_val = y_train[-5000:].astype(np.int32)\n",
        "x_train = x_train[ :-5000]/255.0\n",
        "x_train = np.expand_dims(x_train, -1).astype(np.float32)\n",
        "y_train = y_train[ :-5000].astype(np.int32)  ## by default y_train is not one-hot coded\n",
        "x_test = x_test/255.0\n",
        "x_test = np.expand_dims(x_test, -1).astype(np.float32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "\n",
        "print( x_val.shape )\n",
        "print( x_train.shape )\n",
        "print( x_test.shape )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 28, 28, 1)\n",
            "(55000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hY9f0XauHYX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define some global variables "
      ]
    },
    {
      "metadata": {
        "id": "QOJ9YaI7KcDh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the helper function for plotting images."
      ]
    },
    {
      "metadata": {
        "id": "nYo69jSGGFP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_images( images, labels=None, preds= None  ):\n",
        "  images = np.squeeze(images )\n",
        "  num = images.shape[0]\n",
        "  n_rows = math.floor( math.sqrt( num ))\n",
        "  n_columns = math.ceil(math.sqrt(num))\n",
        "\n",
        "  fig, axes = plt.subplots( n_rows, n_columns )\n",
        "  \n",
        "  if n_rows * n_columns == 1:\n",
        "    axes = [axes]\n",
        "  else:\n",
        "    axes = axes.flat\n",
        "  \n",
        "  for i, ax in enumerate( axes ):\n",
        "    if i < num:\n",
        "      ax.imshow( images[i] )\n",
        "      xlabel= \"\"\n",
        "      if labels is not None:\n",
        "        xlabel += \"True: %d \"%(labels[i])\n",
        "      if preds is not None:\n",
        "        xlabel += \"Pred: %d\"%(preds[i])\n",
        "      ax.set_xlabel(xlabel)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "411qovFhy-EF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Define the function for inputing data to the Estimator"
      ]
    },
    {
      "metadata": {
        "id": "jr9PECWWm0Eu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_input_fn( features, labels, batch_size ):\n",
        "\n",
        "    \"\"\"An input function for training\"\"\"\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    ## shuffle repeat and batch\n",
        "    # Shuffle, repeat.\n",
        "    dataset = dataset.shuffle(1000).repeat()\n",
        "    # TPU specific batch the slices, to make sure that each batch size is divisible by the number of TPU cores\n",
        "    dataset = dataset.batch( batch_size, drop_remainder = True )\n",
        "    # Return the dataset.\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G7gKO1E5Q-N3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval_input_fn(features, labels, batch_size):\n",
        "    \"\"\"An input function for training\"\"\"\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    dataset = dataset.shuffle(1000).repeat()\n",
        "    dataset = dataset.batch(batch_size, drop_remainder = True )\n",
        "    # Return the dataset.\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q6pCeeWMTBp7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_input_fn(features, labels,batch_size):\n",
        "    \"\"\"An input function for training\"\"\"\n",
        "    # Convert the inputs to a Dataset.\n",
        "    # For predict use_tpu should be False, since drop_remainder is False\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    # Return the dataset.\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoupeiiFo1_N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def metric_fn(labels, logits):\n",
        "    \"\"\"Function to return metrics for evaluation.\"\"\"\n",
        "\n",
        "    predicted_classes = tf.argmax(logits, 1)\n",
        "    accuracy = tf.metrics.accuracy(labels=labels,\n",
        "                                   predictions=predicted_classes,\n",
        "                                   name=\"acc_op\")\n",
        "    return {\"accuracy\": accuracy}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HiewctTEVQsf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Self Defined Estimator"
      ]
    },
    {
      "metadata": {
        "id": "TDnPK26tVW-N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # Args:\n",
        "    #\n",
        "    # features: This is the x-arg from the input_fn.\n",
        "    # labels:   This is the y-arg from the input_fn,\n",
        "    #           see e.g. train_input_fn for these two.\n",
        "    # mode:     Either TRAIN, EVAL, or PREDICT\n",
        "    # params:   User-defined hyper-parameters, e.g. learning-rate.\n",
        "    \n",
        "    # Reference to the tensor named \"x\" in the input-function.\n",
        "#     x = features[\"x\"]\n",
        "\n",
        "    ## create the model networks\n",
        "    x = features\n",
        "    # First convolutional layer.\n",
        "    net = tf.layers.conv2d(inputs=x, name='layer_conv1',\n",
        "                           filters=16, kernel_size=5,\n",
        "                           padding='same', activation=tf.nn.relu)\n",
        "    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n",
        "\n",
        "    # Second convolutional layer.\n",
        "    net = tf.layers.conv2d(inputs=net, name='layer_conv2',\n",
        "                           filters=36, kernel_size=5,\n",
        "                           padding='same', activation=tf.nn.relu)\n",
        "    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)    \n",
        "\n",
        "    # Flatten to a 2-rank tensor.\n",
        "    net = tf.contrib.layers.flatten(net)\n",
        "    # Eventually this should be replaced with:\n",
        "    # net = tf.layers.flatten(net)\n",
        "\n",
        "    # First fully-connected / dense layer.\n",
        "    # This uses the ReLU activation function.\n",
        "    net = tf.layers.dense(inputs=net, name='layer_fc1',\n",
        "                          units=128, activation=tf.nn.relu)    \n",
        "\n",
        "    # Second fully-connected / dense layer.\n",
        "    # This is the last layer so it does not use an activation function.\n",
        "    net = tf.layers.dense(inputs=net, name='layer_fc2',units=10)\n",
        "\n",
        "    ## compute logits\n",
        "    # Logits output of the neural network.\n",
        "    logits = net\n",
        "    \n",
        "    ## for training and evaluation\n",
        "    ## compute loss\n",
        "    loss =  tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
        "                                                                       logits=logits))\n",
        "    # Softmax output of the neural network.\n",
        "    y_pred_probabilities = tf.nn.softmax(logits=logits)\n",
        "    # Classification output of the neural network.\n",
        "    y_pred_classes = tf.argmax(y_pred_probabilities, axis=1)[:, tf.newaxis]\n",
        "    # Prediction Accuracy for evaluation\n",
        "    pred_accuracy = tf.metrics.accuracy( labels = labels, predictions = y_pred_classes , name = \"op_acc\" )\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        predictions = {\n",
        "              \"logits\": logits,\n",
        "              \"class_ids\": y_pred_classes,\n",
        "              \"probabilities\": y_pred_probabilities,\n",
        "        }\n",
        "        \n",
        "        spec = tf.contrib.tpu.TPUEstimatorSpec(mode=mode, predictions=predictions)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "        spec = tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=loss, eval_metrics=(metric_fn, [labels, logits] ) )\n",
        "\n",
        "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
        "  \n",
        "        # Define the optimizer for improving the neural network.\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\n",
        "        \n",
        "        if FLAGS.use_tpu:\n",
        "            optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "        train_op = optimizer.minimize(\n",
        "            loss=loss, global_step=tf.train.get_global_step())\n",
        "\n",
        "#         # Define the evaluation metrics,\n",
        "#         # in this case the classification accuracy.\n",
        "#         metrics = \\\n",
        "#         {\n",
        "#             \"accuracy\": tf.metrics.accuracy(labels, y_pred_cls)\n",
        "#         }\n",
        "\n",
        "#         # Wrap all of this in an EstimatorSpec.\n",
        "# #         spec = tf.estimator.EstimatorSpec(\n",
        "# #             mode=mode,\n",
        "# #             loss=loss,\n",
        "# #             train_op=train_op,\n",
        "# #             eval_metric_ops=metrics)\n",
        "        \n",
        "        spec= tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=loss,train_op= train_op, eval_metrics=(metric_fn, [labels, logits] ) )\n",
        "        \n",
        "    return spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iQEvS-dadw0U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "    FLAGS.tpu)\n",
        "\n",
        "# tpu_config = os.environ.get('TF_CONFIG')\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    model_dir=FLAGS.model_dir,\n",
        "    cluster=tpu_cluster_resolver,\n",
        "#     save_checkpoints_steps = 0,\n",
        "    session_config=tf.ConfigProto(\n",
        "        allow_soft_placement=True, log_device_placement=True),\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(FLAGS.iterations),\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ovokb2YRWPG8",
        "colab_type": "code",
        "outputId": "2e95824a-929b-4418-d7f9-037b276e0132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "cell_type": "code",
      "source": [
        "# model = tf.estimator.Estimator(model_fn=model_fn,\n",
        "#                                params={\"learning_rate\": 1e-4},\n",
        "#                                model_dir=\"./checkpoints_tutorial17-2/\")\n",
        "\n",
        "model = tf.contrib.tpu.TPUEstimator(\n",
        "                               model_fn=model_fn,\n",
        "                               params = {\"learning_rate\": 1e-4 },\n",
        "                               config = run_config,\n",
        "                               use_tpu= FLAGS.use_tpu,\n",
        "                               train_batch_size=FLAGS.batch_size,\n",
        "                               eval_batch_size=FLAGS.batch_size,\n",
        "                               predict_batch_size=FLAGS.batch_size,\n",
        "  \n",
        "                               \n",
        "                                )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://carlos-vic.appspot.com', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "log_device_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.11.28.90:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb18704b00>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.11.28.90:8470', '_evaluation_master': 'grpc://10.11.28.90:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fcb18a0e860>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "odBUseLSWYtl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "2ON1-FUS7VoX",
        "colab_type": "code",
        "outputId": "cfcd3bb3-2f2d-455b-be1d-13c41cf777fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "!gsutil mb gs://carlos-vic.appspot.com"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating gs://carlos-vic.appspot.com/...\n",
            "You are attempting to perform an operation that requires a project id, with none configured. Please re-run gsutil config and make sure to follow the instructions for finding and entering your default project id.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g9pPDsivykHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t8dY5URVWa5x",
        "colab_type": "code",
        "outputId": "c0a1cdd6-abef-4f4b-89d7-5d41ad912205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1019
        }
      },
      "cell_type": "code",
      "source": [
        "model.train( input_fn = lambda params: train_input_fn( x_train, y_train, params[\"batch_size\"] ), steps=2000  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://carlos-vic.appspot.com/model.ckpt-100000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 100000 into gs://carlos-vic.appspot.com/model.ckpt.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 7 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 2.8953691e-05, step = 100500\n",
            "INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 4.0757136e-06, step = 101000 (0.756 sec)\n",
            "INFO:tensorflow:global_step/sec: 660.996\n",
            "INFO:tensorflow:examples/sec: 84607.5\n",
            "INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.0, step = 101500 (0.570 sec)\n",
            "INFO:tensorflow:global_step/sec: 876.715\n",
            "INFO:tensorflow:examples/sec: 112220\n",
            "INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.0, step = 102000 (0.546 sec)\n",
            "INFO:tensorflow:global_step/sec: 916.336\n",
            "INFO:tensorflow:examples/sec: 117291\n",
            "INFO:tensorflow:Saving checkpoints for 102000 into gs://carlos-vic.appspot.com/model.ckpt.\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Loss for final step: 0.0.\n",
            "INFO:tensorflow:training_loop marked as finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.contrib.tpu.python.tpu.tpu_estimator.TPUEstimator at 0x7fcb20e39f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "2-TdmDsUtAN8",
        "colab_type": "code",
        "outputId": "a35340ca-b7b9-4e7d-b858-23a590aeb717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        }
      },
      "cell_type": "code",
      "source": [
        "eval_result = model.evaluate(\n",
        "      input_fn=lambda params: eval_input_fn(\n",
        "          x_val, y_val, params[\"batch_size\"]),\n",
        "      steps=FLAGS.eval_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-04-08T12:43:37Z\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://carlos-vic.appspot.com/model.ckpt-102000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 9 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (4) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (4) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Evaluation [4/4]\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Finished evaluation at 2019-04-08-12:43:49\n",
            "INFO:tensorflow:Saving dict for global step 102000: accuracy = 0.9902344, global_step = 102000, loss = 8.535862e-05\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 102000: gs://carlos-vic.appspot.com/model.ckpt-102000\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oo_STluG0nVU",
        "colab_type": "code",
        "outputId": "2d4b3f8a-93f0-421c-a84a-fe79604feb99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "predict_result = model.predict(\n",
        "      input_fn=lambda params: predict_input_fn(\n",
        "          x_test,y_test, params[\"batch_size\"]))\n",
        "\n",
        "\n",
        "next(predict_result)\n",
        "\n",
        "# for pred_dict, expec in zip( predict_result, y_test  ):\n",
        "#   class_id = pred_dict[\"class_ids\"][0]\n",
        "#   prob_pred = pred_dict[\"probabilities\"][class_id]\n",
        "#   print( \"Prediction is %d, probility %.1f, expected %d\" %( class_id, prob_pred, expec )  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://carlos-vic.appspot.com/model.ckpt-102000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 7 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class_ids': array([7]),\n",
              " 'logits': array([-18.951256  ,   1.2203839 ,  -0.69914997,   2.895075  ,\n",
              "        -35.774868  , -12.417238  , -79.63498   ,  50.539818  ,\n",
              "          4.910502  ,   6.9981675 ], dtype=float32),\n",
              " 'probabilities': array([6.6131789e-31, 3.8092727e-22, 5.5872563e-23, 2.0330702e-21,\n",
              "        3.2659478e-38, 4.5509267e-28, 0.0000000e+00, 1.0000000e+00,\n",
              "        1.5256014e-20, 1.2305580e-19], dtype=float32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "aEotpUEtP4_A",
        "colab_type": "code",
        "outputId": "2f29bca5-e2bb-489a-9966-16cad7cf922a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "next(predict_result, 10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class_ids': array([5]),\n",
              " 'logits': array([-24.94518 , -49.249718, -20.357456, -28.440752, -11.425112,\n",
              "         35.63763 ,  17.286167, -20.439623,  10.581571, -12.081831],\n",
              "       dtype=float32),\n",
              " 'probabilities': array([4.88899473e-27, 1.36111573e-37, 4.80442570e-25, 1.48289985e-28,\n",
              "        3.63839229e-21, 1.00000000e+00, 1.07167075e-08, 4.42545347e-25,\n",
              "        1.31307925e-11, 1.88669861e-21], dtype=float32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "K4h4esNSuMEU",
        "colab_type": "code",
        "outputId": "fa732440-4434-4937-8243-3c593f3f4b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "predict_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object TPUEstimator.predict at 0x7f05c0106990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "k80CFD898yIK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#  you may not use this file except in compliance with the License.\n",
        "#  You may obtain a copy of the License at\n",
        "#\n",
        "#   http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#  Unless required by applicable law or agreed to in writing, software\n",
        "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#  See the License for the specific language governing permissions and\n",
        "#  limitations under the License.\n",
        "\n",
        "\"\"\"Module to generate Iris dataset for using in custom TPUEstimator.\"\"\"\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "TRAIN_URL = 'http://download.tensorflow.org/data/iris_training.csv'\n",
        "TEST_URL = 'http://download.tensorflow.org/data/iris_test.csv'\n",
        "\n",
        "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
        "                    'PetalLength', 'PetalWidth', 'Species']\n",
        "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
        "\n",
        "PREDICTION_INPUT_DATA = {\n",
        "    'SepalLength': [6.9, 5.1, 5.9],\n",
        "    'SepalWidth': [3.1, 3.3, 3.0],\n",
        "    'PetalLength': [5.4, 1.7, 4.2],\n",
        "    'PetalWidth': [2.1, 0.5, 1.5],\n",
        "}\n",
        "\n",
        "PREDICTION_OUTPUT_DATA = ['Virginica', 'Setosa', 'Versicolor']\n",
        "\n",
        "\n",
        "def maybe_download():\n",
        "  train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\n",
        "  test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
        "\n",
        "  return train_path, test_path\n",
        "\n",
        "\n",
        "def load_data(y_name='Species'):\n",
        "  \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\n",
        "  train_path, test_path = maybe_download()\n",
        "\n",
        "  train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0,\n",
        "                      dtype={'SepalLength': pd.np.float32,\n",
        "                             'SepalWidth': pd.np.float32,\n",
        "                             'PetalLength': pd.np.float32,\n",
        "                             'PetalWidth': pd.np.float32,\n",
        "                             'Species': pd.np.int32})\n",
        "  train_x, train_y = train, train.pop(y_name)\n",
        "\n",
        "  test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0,\n",
        "                     dtype={'SepalLength': pd.np.float32,\n",
        "                            'SepalWidth': pd.np.float32,\n",
        "                            'PetalLength': pd.np.float32,\n",
        "                            'PetalWidth': pd.np.float32,\n",
        "                            'Species': pd.np.int32})\n",
        "  test_x, test_y = test, test.pop(y_name)\n",
        "\n",
        "  return (train_x, train_y), (test_x, test_y)\n",
        "\n",
        "\n",
        "def train_input_fn(features, labels, batch_size):\n",
        "  \"\"\"An input function for training.\"\"\"\n",
        "\n",
        "  # Convert the inputs to a Dataset.\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "  # Shuffle, repeat, and batch the examples.\n",
        "  dataset = dataset.shuffle(1000).repeat()\n",
        "\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  # Return the dataset.\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def eval_input_fn(features, labels, batch_size):\n",
        "  \"\"\"An input function for evaluation.\"\"\"\n",
        "  features = dict(features)\n",
        "  inputs = (features, labels)\n",
        "\n",
        "  # Convert the inputs to a Dataset.\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "  dataset = dataset.shuffle(1000).repeat()\n",
        "\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  # Return the dataset.\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def predict_input_fn(features, batch_size):\n",
        "  \"\"\"An input function for prediction.\"\"\"\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(features)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j06wysEPVXzX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Prediction"
      ]
    },
    {
      "metadata": {
        "id": "FSQN1XAkC6I3",
        "colab_type": "code",
        "outputId": "aba8bb37-5bb8-4dd2-98bd-4e05e3bfd36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "test_results = model.evaluate( input_fn = test_input_fn )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2abcefce3cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_input_fn\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_input_fn' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "roHQ3DkTQo6G",
        "colab_type": "code",
        "outputId": "1df12d8b-678c-44d1-acfd-e8a3e6aec961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "test_results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.977,\n",
              " 'average_loss': 0.083602235,\n",
              " 'global_step': 2000,\n",
              " 'loss': 10.5825615}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "metadata": {
        "id": "ER_E-9mUQrUZ",
        "colab_type": "code",
        "outputId": "d8ead1f4-4671-4569-e3a2-c1c9b2d4dcd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Classification accuracy: %.2f%%\"%(test_results[\"accuracy\"] * 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification accuracy: 97.70%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FD8NQGeWRJ7e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}